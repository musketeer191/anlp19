{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook explores word embeddings through the functionality of Gensim; we train new embeddings from a dataset of our own and compare with pre-trained Glove embeddings.\n",
    "\n",
    "Before getting started, install the gensim library:\n",
    "\n",
    "```sh\n",
    "conda install gensim=3.4.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T17:16:42.585410Z",
     "start_time": "2020-07-16T17:16:18.271308Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\victor\\anaconda3\\envs\\updated\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\victor\\anaconda3\\envs\\updated\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\victor\\anaconda3\\envs\\updated\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.21-py2.py3-none-any.whl (128 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\victor\\anaconda3\\envs\\updated\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.21\n",
      "  Downloading botocore-1.17.21-py2.py3-none-any.whl (6.3 MB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\victor\\anaconda3\\envs\\updated\\lib\\site-packages (from botocore<1.18.0,>=1.17.21->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110324 sha256=793a8b02ef3d9f014f32bee2717a0e7c9418be0ce03b9c9a1aad3e4ed29a6d37\n",
      "  Stored in directory: c:\\users\\victor\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, urllib3, idna, chardet, requests, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed Cython-0.29.14 boto-2.49.0 boto3-1.14.21 botocore-1.17.21 chardet-3.0.4 docutils-0.15.2 gensim-3.8.3 idna-2.10 jmespath-0.10.0 requests-2.24.0 s3transfer-0.3.3 smart-open-2.1.0 urllib3-1.25.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:57:56.007513Z",
     "start_time": "2020-08-04T22:57:54.225021Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in c:\\users\\victor\\anaconda3\\envs\\nlp\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\victor\\anaconda3\\envs\\nlp\\lib\\site-packages (from fasttext) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\victor\\anaconda3\\envs\\nlp\\lib\\site-packages (from fasttext) (47.3.0.post20200616)\n",
      "Requirement already satisfied: numpy in c:\\users\\victor\\anaconda3\\envs\\nlp\\lib\\site-packages (from fasttext) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T22:58:35.872739Z",
     "start_time": "2020-08-04T22:58:27.805665Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train a new word2vec model on our data. As the wiki data has some bug with unicode decoding, I switch to brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:26:38.866841Z",
     "start_time": "2020-07-17T04:26:38.863221Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:26:53.907070Z",
     "start_time": "2020-07-17T04:26:51.584313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences in Brown corpus: 57340\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown as brown_corp\n",
    "\n",
    "sentences = brown_corp.sents()\n",
    "print('# sentences in Brown corpus:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T23:31:33.380938Z",
     "start_time": "2020-07-16T23:31:33.276695Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2963: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0b5dc6ef90af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../data/wiki.10K.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# replace any sequence of whitespace (space, tab, newline, etc.) with single space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\updated\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2963: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# sentences=[]\n",
    "# filename=\"../data/wiki.10K.txt\"\n",
    "# with open(filename) as file:\n",
    "#     for line in file:\n",
    "#         words=line.rstrip().lower()\n",
    "#         # replace any sequence of whitespace (space, tab, newline, etc.) with single space\n",
    "#         words=re.sub(\"\\s+\", \" \", words)\n",
    "#         sentences.append(words.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trained via CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:27:34.885546Z",
     "start_time": "2020-07-17T04:27:08.990249Z"
    }
   },
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(\n",
    "        sentences,\n",
    "        size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        sg=0\n",
    ")\n",
    "\n",
    "cbow_trained_vectors = cbow_model.wv\n",
    "# save vectors to file if you want to use them later\n",
    "cbow_trained_vectors.save_word2vec_format('cbow_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trained via Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:12.510239Z",
     "start_time": "2020-07-17T04:27:34.887534Z"
    }
   },
   "outputs": [],
   "source": [
    "sg_model = Word2Vec(sentences, \n",
    "                    size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        sg=1\n",
    ")\n",
    "sg_trained_vectors = sg_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:15.818402Z",
     "start_time": "2020-07-17T04:28:12.513231Z"
    }
   },
   "outputs": [],
   "source": [
    "# save embeddings to file for re-use\n",
    "sg_trained_vectors.save_word2vec_format('sg_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:15.876725Z",
     "start_time": "2020-07-17T04:28:15.820358Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('priest', 0.9478057622909546),\n",
       " ('critic', 0.9460633993148804),\n",
       " ('painter', 0.9407342672348022),\n",
       " ('apprentice', 0.9377787709236145),\n",
       " ('fame', 0.9376294612884521),\n",
       " ('dancer', 0.9374251365661621),\n",
       " ('masterpiece', 0.9358892440795898),\n",
       " ('suggestion', 0.9349619150161743),\n",
       " ('servant', 0.9344593286514282),\n",
       " ('Being', 0.9334083199501038)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_trained_vectors.most_similar('actor', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:15.908639Z",
     "start_time": "2020-07-17T04:28:15.878723Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('missionary', 0.9562637805938721),\n",
       " ('prohibition', 0.9549413919448853),\n",
       " ('commander', 0.9545937180519104),\n",
       " ('proposal', 0.9542382955551147),\n",
       " ('contract', 0.9517210721969604),\n",
       " ('Commission', 0.9508786201477051),\n",
       " ('fame', 0.9494434595108032),\n",
       " ('prize', 0.9459772706031799),\n",
       " ('incomplete', 0.9456023573875427),\n",
       " ('acceptance', 0.944739818572998)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_trained_vectors.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obs:__\n",
    "\n",
    "+ `fame` appear in both top10 lists\n",
    "+ skip-gram mode seems to find out rare words like `opera`, `servant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:15.918612Z",
     "start_time": "2020-07-17T04:28:15.910633Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scientist', 0.9173941612243652),\n",
       " ('manufacturer', 0.9163181185722351),\n",
       " ('actor', 0.9149243831634521),\n",
       " ('magazine', 0.9141924977302551),\n",
       " ('commission', 0.9136365652084351),\n",
       " ('award', 0.9118192195892334),\n",
       " ('prize', 0.9062156081199646),\n",
       " ('Englishman', 0.9060776233673096),\n",
       " ('Orthodox', 0.9049038887023926),\n",
       " ('critic', 0.9046592712402344)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_trained_vectors.most_similar('engineer', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:28:15.929582Z",
     "start_time": "2020-07-17T04:28:15.920606Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soldier', 0.9836461544036865),\n",
       " ('attorney', 0.961202085018158),\n",
       " ('prize', 0.9600339531898499),\n",
       " ('English', 0.9591624736785889),\n",
       " ('description', 0.9585482478141785),\n",
       " ('Foundation', 0.9573974013328552),\n",
       " ('tie', 0.9568837285041809),\n",
       " ('strain', 0.956856906414032),\n",
       " ('former', 0.9559668898582458),\n",
       " ('impersonal', 0.9555135369300842)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_trained_vectors.most_similar(\"engineer\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on bigger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T01:55:35.116165Z",
     "start_time": "2020-07-17T01:55:29.934649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # download data\n",
    "# import os.path\n",
    "# if not os.path.isfile('text8'):\n",
    "#     !wget -c http://mattmahoney.net/dc/text8.zip\n",
    "#     !unzip text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:36:33.685375Z",
     "start_time": "2020-07-17T04:36:33.680206Z"
    },
    "code_folding": [
     12
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 5\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "# Same values as used for fastText training above\n",
    "params = {\n",
    "    'alpha': lr,\n",
    "    'size': dim,\n",
    "    'window': ws,\n",
    "    'iter': epoch,\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 1, # use hierarchical softmax for model training\n",
    "#     'negative': neg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:46:27.798837Z",
     "start_time": "2020-07-17T04:38:34.156086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "corpus_file = '../data/text8'\n",
    "\n",
    "%time gs_model = Word2Vec(Text8Corpus(corpus_file), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T04:46:27.859569Z",
     "start_time": "2020-07-17T04:46:27.798837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.9190559387207031),\n",
       " ('comedian', 0.8873367309570312),\n",
       " ('singer', 0.858544647693634),\n",
       " ('musician', 0.830625593662262),\n",
       " ('american', 0.8068164587020874),\n",
       " ('screenwriter', 0.8065409064292908),\n",
       " ('footballer', 0.799182653427124),\n",
       " ('monkhouse', 0.7879550457000732),\n",
       " ('playwright', 0.7821558713912964),\n",
       " ('tobey', 0.7811105251312256)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_vectors = gs_model.wv\n",
    "gs_vectors.most_similar('actor', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T06:09:51.301067Z",
     "start_time": "2020-07-17T06:09:46.653439Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_vectors.save_word2vec_format('gensim_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with pre-trained Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in vectors that have already been trained on a much bigger dataset. [Glove vectors](https://nlp.stanford.edu/projects/glove/) are trained using a different method than word2vec, but results in vectors that can be read in by Gensim.  The top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to convert the Glove format into w2v format; this creates a new file\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(glove_in_w2v_format, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`most_similar` allows for vector arithmetic (as the average value of the input positive/negative vectors, where negative vectors are first multiplied by -1).  Play around with this function to discover other analogies that have been learned in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one + two = three + ?\n",
    "one=\"man\"\n",
    "two=\"king\"\n",
    "three=\"woman\"\n",
    "\n",
    "one=\"paris\"\n",
    "two=\"france\"\n",
    "three=\"berlin\"\n",
    "\n",
    "glove.most_similar(positive=[two, three], negative=[one], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the quality of the learned vectors through an intrinsic evaluation comparing to human judgments in the wordsim 353 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_vectors.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
