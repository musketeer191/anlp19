{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook explores distribitional simliarity in a dataset of 10,000 Wikipedia articles (4.4M words), building high-dimensional, sparse representations for words from the distinct contexts they appear in.  These representations allow for analysis of the most similar words to a given query, and are interpretable with respect to the specific contexts that are most important for determining that two words are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:24:15.695317Z",
     "start_time": "2020-07-15T23:24:15.689359Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import operator\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:24:15.706724Z",
     "start_time": "2020-07-15T23:24:15.698764Z"
    }
   },
   "outputs": [],
   "source": [
    "window=2\n",
    "vocabSize=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:13:09.872698Z",
     "start_time": "2020-07-16T00:13:09.276261Z"
    }
   },
   "outputs": [],
   "source": [
    "filename=\"../data/wiki.10K.txt\"\n",
    "\n",
    "wiki_data=open(filename, encoding=\"utf-8\").read().lower().split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:39:56.406598Z",
     "start_time": "2020-07-15T23:39:56.402607Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:41:19.922367Z",
     "start_time": "2020-07-15T23:41:19.916383Z"
    }
   },
   "outputs": [],
   "source": [
    "puncts = string.punctuation\n",
    "en_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:13:32.504148Z",
     "start_time": "2020-07-16T00:13:27.038451Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_wiki_data = [word for word in wiki_data if (word not in puncts) and \n",
    "             (word not in en_stop_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:25:07.359955Z",
     "start_time": "2020-07-15T23:25:07.352977Z"
    }
   },
   "outputs": [],
   "source": [
    "# We'll only create word representation for the most frequent K words\n",
    "\n",
    "def create_vocab(data):\n",
    "    word_representations={}\n",
    "    vocab=Counter()\n",
    "    for i, word in enumerate(data):\n",
    "        vocab[word]+=1\n",
    "\n",
    "    topK=[k for k,v in vocab.most_common(vocabSize)]\n",
    "    for k in topK:\n",
    "        word_representations[k]=defaultdict(float)\n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:26:09.066688Z",
     "start_time": "2020-07-15T23:26:09.061699Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# word representation for a word = its unigram distributional context (the unigrams that show\n",
    "# up in a window before and after its occurence)\n",
    "\n",
    "def count_unigram_context(data, word_representations):\n",
    "    for i, word in enumerate(data):\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                word_representations[word][data[j]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:26:13.743955Z",
     "start_time": "2020-07-15T23:26:13.735981Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_directional_context(data, word_representations):\n",
    "    for i, word in enumerate(data):\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        left=\"L: %s\" % ' '.join(data[start:i])\n",
    "        right=\"R: %s\" % ' '.join(data[i+1:end])\n",
    "        \n",
    "        word_representations[word][left]+=1\n",
    "        word_representations[word][right]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:28:22.299905Z",
     "start_time": "2020-07-16T00:28:22.295918Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:29:56.762793Z",
     "start_time": "2020-07-16T00:29:56.755811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(list({'a': 1, 'b': 2}.values()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:31:41.199299Z",
     "start_time": "2020-07-16T00:31:41.193317Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total = sum(np.power(list(word_representations[word].values()), 2))\n",
    "#         total=0\n",
    "#         for key in word_representations[word]:\n",
    "#             total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "    \n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:27:37.124829Z",
     "start_time": "2020-07-15T23:27:37.119732Z"
    }
   },
   "outputs": [],
   "source": [
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:27:47.609120Z",
     "start_time": "2020-07-15T23:27:47.604703Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:27:55.968593Z",
     "start_time": "2020-07-15T23:27:55.963532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram vs. directional contexts\n",
    "\n",
    "Explore the difference between `count_unigram_context` and `count_directional_context` for determining what counts as \"context\".  `count_unigram_context` counts an individual unigram in the bag of words around a target as a \"context\" variable, while `count_directional_context` counts the sequence of words before and after the word as a single \"context\"--and specifies the direction it occurs (to the left or right of the word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:14:19.940229Z",
     "start_time": "2020-07-16T00:14:08.550734Z"
    }
   },
   "outputs": [],
   "source": [
    "word_representations=create_vocab(wiki_data)\n",
    "count_unigram_context(wiki_data, word_representations)\n",
    "word_representations = normalize(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:14:20.320257Z",
     "start_time": "2020-07-16T00:14:19.943113Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tactor\t1.00000\n",
      "1\tactress\t0.94363\n",
      "2\tartist\t0.88560\n",
      "3\twriter\t0.85602\n",
      "4\tpolitician\t0.84846\n",
      "5\tmusician\t0.84787\n",
      "6\tentrepreneur\t0.84640\n",
      "7\tengineer\t0.83586\n",
      "8\tsinger\t0.82831\n",
      "9\tactivist\t0.82257\n"
     ]
    }
   ],
   "source": [
    "find_nearest_neighbors(word_representations, \"actor\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, `politician`, `entrepreneur`, `activist` and even `engineer` are near neighbors of `actor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:29:51.619451Z",
     "start_time": "2020-07-15T23:29:51.612470Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Let's find the contexts shared between two words that have the most contribution\n",
    "# to the cosine similarity\n",
    "\n",
    "def find_shared_contexts(word_representations, query1, query2, K):\n",
    "    if query1 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query1)\n",
    "        return None\n",
    "    \n",
    "    if query2 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query2)\n",
    "        return None\n",
    "    \n",
    "    context_scores={}\n",
    "    dict1=word_representations[query1]\n",
    "    dict2=word_representations[query2]\n",
    "    \n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            score=dict1[key]*dict2[key]\n",
    "            context_scores[key]=score\n",
    "\n",
    "    sorted_x = sorted(context_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "        print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:16:35.365049Z",
     "start_time": "2020-07-16T00:16:35.359559Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tand\t0.22555\n",
      "1\t.\t0.18242\n",
      "2\ta\t0.10453\n",
      "3\t,\t0.09911\n",
      "4\tan\t0.07981\n",
      "5\tthe\t0.05109\n",
      "6\the\t0.01720\n",
      "7\tin\t0.01608\n",
      "8\tamerican\t0.01511\n",
      "9\t(\t0.01307\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:16:09.905377Z",
     "start_time": "2020-07-16T00:16:09.899392Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tand\t0.23554\n",
      "1\t,\t0.18507\n",
      "2\t.\t0.13991\n",
      "3\tan\t0.09476\n",
      "4\ta\t0.05330\n",
      "5\tthe\t0.04516\n",
      "6\tin\t0.01579\n",
      "7\the\t0.01555\n",
      "8\tby\t0.00987\n",
      "9\t(\t0.00962\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"entrepreneur\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that these word pairs share mostly stop words and punctuation. Let us strip them to see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:22:08.750082Z",
     "start_time": "2020-07-16T00:22:08.743075Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a helper to avoid code repetition\n",
    "def cal_normalized_repr(text_data):\n",
    "    word_representations=create_vocab(text_data)\n",
    "    count_unigram_context(text_data, word_representations)\n",
    "    return normalize(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:33:18.597136Z",
     "start_time": "2020-07-16T00:33:11.565910Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_word_repr = cal_normalized_repr(cleaned_wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:36:00.521438Z",
     "start_time": "2020-07-16T00:36:00.514472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tamerican\t0.09663\n",
      "1\tborn\t0.01678\n",
      "2\tindian\t0.00814\n",
      "3\tbritish\t0.00678\n",
      "4\tcanadian\t0.00559\n",
      "5\tserved\t0.00542\n",
      "6\tenglish\t0.00415\n",
      "7\t''\t0.00373\n",
      "8\tdirector\t0.00322\n",
      "9\t's\t0.00280\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(clean_word_repr, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T00:36:28.350397Z",
     "start_time": "2020-07-16T00:36:28.344888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tbest\t0.03721\n",
      "1\t''\t0.02212\n",
      "2\tfilm\t0.01609\n",
      "3\tamerican\t0.01509\n",
      "4\tactor\t0.01207\n",
      "5\tknown\t0.01106\n",
      "6\tdirector\t0.00955\n",
      "7\tcanadian\t0.00905\n",
      "8\tborn\t0.00905\n",
      "9\t``\t0.00654\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(clean_word_repr, \"actor\", \"entrepreneur\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start seeing some logic here:\n",
    "+ actor and entrepreneur co-occur with *best* and *known*, make sense because wiki articles talk about best/well-known persons\n",
    "\n",
    "+ actor and entrepreneur also co-occur with *director*, maybe some famous actors also become director/movie entrepreneur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:46:04.607452Z",
     "start_time": "2020-07-15T23:45:57.904535Z"
    }
   },
   "outputs": [],
   "source": [
    "word_representations=create_vocab(wiki_data)\n",
    "count_directional_context(wiki_data, word_representations)\n",
    "normalize(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:51:04.166716Z",
     "start_time": "2020-07-15T23:51:03.913469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tactor\t1.00000\n",
      "1\tactress\t0.30555\n",
      "2\tperhaps\t0.18018\n",
      "3\tfilmmaker\t0.15327\n",
      "4\tscreenwriter\t0.13743\n",
      "5\twriter\t0.10975\n",
      "6\tproducer\t0.09931\n",
      "7\tprobably\t0.07869\n",
      "8\tmusician\t0.07860\n",
      "9\tconsultant\t0.07101\n"
     ]
    }
   ],
   "source": [
    "find_nearest_neighbors(word_representations, 'actor', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:51:58.332899Z",
     "start_time": "2020-07-15T23:51:58.327533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tR: new york\t0.00383\n",
      "1\tL: 1973 american\t0.00191\n",
      "2\tL: 1942 american\t0.00096\n",
      "3\tR: appeared tamil\t0.00096\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:55:24.669747Z",
     "start_time": "2020-07-15T23:55:24.664753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tR: best known\t0.13411\n",
      "1\tR: film director\t0.01916\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"filmmaker\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T23:56:30.813706Z",
     "start_time": "2020-07-15T23:56:30.808722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tR: best known\t0.07101\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"consultant\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
