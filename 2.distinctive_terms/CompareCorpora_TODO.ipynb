{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook explores methods for comparing two different textual datasets to identify the terms that are distinct to each one:\n",
    "\n",
    "* Difference of proportions (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) section 3.2.2\n",
    "* Mann-Whitney rank-sums test (described in [Kilgarriff 2001, Comparing Corpora](https://www.sketchengine.eu/wp-content/uploads/comparing_corpora_2001.pdf), section 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:04:10.056460Z",
     "start_time": "2020-07-15T03:04:10.053452Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:58:47.794164Z",
     "start_time": "2020-07-15T02:58:47.790584Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:47:00.480231Z",
     "start_time": "2020-07-15T02:47:00.414405Z"
    }
   },
   "outputs": [],
   "source": [
    "# the convote data is already tokenized so just split on whitespace\n",
    "repub_tokens=open(\"../data/repub.convote.txt\", encoding=\"utf-8\").read().split(\" \")\n",
    "dem_tokens=open(\"../data/dem.convote.txt\", encoding=\"utf-8\").read().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: First, calculate the simple \"difference of proportions\" measure from Monroe et al.'s \"Fighting Words\", section 3.2.2.  What are the top ten terms in this measurement that are most republican and most democrat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:56:08.931655Z",
     "start_time": "2020-07-15T02:56:08.925412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republic tokens:  416714\n",
      "democrat tokens:  483171\n"
     ]
    }
   ],
   "source": [
    "print('republic tokens: ', len(repub_tokens))\n",
    "print('democrat tokens: ', len(dem_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:43:43.988371Z",
     "start_time": "2020-07-15T03:43:43.694116Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:40:39.240609Z",
     "start_time": "2020-07-15T03:40:39.235621Z"
    }
   },
   "source": [
    "## Clean text \n",
    "\n",
    "+ remove punct,\n",
    "+ remove stop words, \n",
    "+ stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:46:42.689307Z",
     "start_time": "2020-07-15T03:46:42.682310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "puncts = string.punctuation\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:48:23.787906Z",
     "start_time": "2020-07-15T03:48:22.418014Z"
    }
   },
   "outputs": [],
   "source": [
    "repub_tokens = [word for word in repub_tokens if (word not in puncts) and (word not in stop_words)]\n",
    "dem_tokens = [word for word in dem_tokens if (word not in puncts) and (word not in stop_words) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:52:41.369705Z",
     "start_time": "2020-07-15T03:52:34.067390Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "repub_tokens_stemmed = [ps.stem(word) for word in repub_tokens]\n",
    "dem_tokens_stemmed = [ps.stem(word) for word in dem_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:07:06.988371Z",
     "start_time": "2020-07-15T03:07:06.983655Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_counts(tokens):\n",
    "    counts = defaultdict(int)\n",
    "    for word in tokens:\n",
    "        counts[word] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:35:36.614118Z",
     "start_time": "2020-07-15T03:35:36.606139Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def chi_square(one_counts, two_counts):\n",
    "\n",
    "    one_sum, two_sum = sum(one_counts.values()), sum(two_counts.values())\n",
    "    vocab = list(np.union1d(list(one_counts.keys()), list(two_counts.keys())))\n",
    "    \n",
    "    N=one_sum + two_sum\n",
    "    vals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        O11=one_counts[word]\n",
    "        O12=two_counts[word]\n",
    "        O21=one_sum-one_counts[word]\n",
    "        O22=two_sum-two_counts[word]\n",
    "        \n",
    "        # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "        # for 2x2 contingency tables: \n",
    "        # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "        vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11 + O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "        \n",
    "    sorted_chi = {k:v for (k, v) in sorted(vals.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    return sorted_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:36:44.549540Z",
     "start_time": "2020-07-15T03:36:44.542556Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def difference_of_proportions(one_tokens, two_tokens, one_cls_name, two_cls_name):\n",
    "#     one_counts = dict(Counter(one_tokens))\n",
    "#     two_counts = dict(Counter(two_tokens))\n",
    "    one_counts, two_counts = cal_counts(one_tokens), cal_counts(two_tokens)\n",
    "    sorted_chi = chi_square(one_counts, two_counts)\n",
    "    \n",
    "    one_sum, two_sum = sum(one_counts.values()), sum(two_counts.values())\n",
    "    one=[]\n",
    "    two=[]\n",
    "    for k in sorted_chi:\n",
    "        if one_counts[k]/one_sum > two_counts[k]/two_sum:\n",
    "            one.append(k)\n",
    "        else:\n",
    "            two.append(k)\n",
    "    \n",
    "    print (one_cls_name, ':')\n",
    "    for k in one[:20]:\n",
    "        print(\"%s\\t%s\" % (k, sorted_chi[k]))\n",
    "\n",
    "    print (\"\\n\\n{}:\\n\".format(two_cls_name))\n",
    "    for k in two[:20]:\n",
    "        print(\"%s\\t%s\" % (k, sorted_chi[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:48:46.532500Z",
     "start_time": "2020-07-15T03:48:46.379710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat :\n",
      "cuts\t354.76394914480903\n",
      "republican\t315.9149660695275\n",
      "billion\t159.20533317280237\n",
      "cut\t157.1287194169713\n",
      "republicans\t135.55603537338268\n",
      "--\t124.12411856122874\n",
      "cbc\t120.72444361187453\n",
      "majority\t117.69870422458176\n",
      "administration\t109.04797567950855\n",
      "debt\t103.7928198661121\n",
      "budget\t103.11718636395187\n",
      "iraq\t100.08594190293657\n",
      "professor\t92.85743126761008\n",
      "theresa\t76.17157546435062\n",
      "social\t71.55265790848058\n",
      "health\t70.27991663441924\n",
      "fails\t67.9300074948242\n",
      "gun\t64.97365346386344\n",
      "university\t62.674555431170695\n",
      "n't\t61.11643010254004\n",
      "\n",
      "\n",
      "Republic:\n",
      "\n",
      "gang\t135.28114375995779\n",
      "economy\t112.76583946480014\n",
      "chairman\t100.647013372887\n",
      "growth\t100.64442459346924\n",
      "small\t92.22274660863798\n",
      "gentleman\t88.93870285590893\n",
      "businesses\t83.0519443942429\n",
      "jurisdiction\t82.3714935212654\n",
      "gangs\t79.87221745586834\n",
      "shall\t78.66749657131065\n",
      "may\t71.68530395687134\n",
      "identification\t70.70678819490136\n",
      "driver\t70.019411279093\n",
      "terrorists\t69.97493533648\n",
      "terri\t69.31482330636425\n",
      "important\t68.38123341198757\n",
      "jobs\t67.0485231904696\n",
      "committee\t63.41542799397627\n",
      "business\t56.18993538883888\n",
      "lawyers\t55.0029203314431\n"
     ]
    }
   ],
   "source": [
    "difference_of_proportions(dem_tokens, repub_tokens, 'Democrat', 'Republic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T03:53:16.143362Z",
     "start_time": "2020-07-15T03:53:16.017617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat :\n",
      "cut\t542.0320077518245\n",
      "republican\t450.46285526417677\n",
      "billion\t175.59634023664373\n",
      "--\t124.12411856122874\n",
      "cbc\t120.72444361187453\n",
      "budget\t107.9492917233775\n",
      "debt\t107.4225725285551\n",
      "fail\t101.2131313048445\n",
      "iraq\t100.08594190293657\n",
      "administr\t93.54095075691816\n",
      "major\t88.61140898053648\n",
      "worker\t81.95803623310654\n",
      "theresa\t76.17157546435062\n",
      "deficit\t75.7730502383019\n",
      "professor\t73.9392811615788\n",
      "gun\t71.04419319448873\n",
      "health\t70.63352853794076\n",
      "opposit\t67.75518061801439\n",
      "social\t67.52269487511651\n",
      "altern\t64.98553647027717\n",
      "\n",
      "\n",
      "Republic:\n",
      "\n",
      "gang\t214.72196763004575\n",
      "busi\t138.72507579817227\n",
      "economi\t112.25884490634984\n",
      "jurisdict\t110.54764809170459\n",
      "chairman\t100.647013372887\n",
      "growth\t100.64442459346924\n",
      "small\t92.22274660863798\n",
      "gentleman\t88.93870285590893\n",
      "terrorist\t82.58421978219984\n",
      "shall\t78.66749657131065\n",
      "licens\t72.87412934485505\n",
      "identif\t71.75229849477107\n",
      "may\t71.68530395687134\n",
      "committe\t69.9531549528938\n",
      "import\t69.57009824196267\n",
      "terri\t68.97625501070368\n",
      "grow\t59.18992687464742\n",
      "look\t57.58220448064527\n",
      "thank\t51.969161258649436\n",
      "driver\t49.97652407729459\n"
     ]
    }
   ],
   "source": [
    "difference_of_proportions(dem_tokens_stemmed, repub_tokens_stemmed, 'Democrat', 'Republic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply analyzing the difference in relative frequencies has a number of downsides: 1.) As Monroe et al (2009) points out (and we can see here as well), it tends to emphasize high-frequency words (be sure you understand why).  2.) We're not measuring whether a difference is statistically meaningful or just due to chance; the $\\chi^2$ test is one method (described in Kilgarriff 2001 and in the context of collocations in Manning and Schuetze [here](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)) that addresses the desideratum of finding statistically significant terms, but it too has another downside: 3.) Simply counting up the total number of mentions of a term doesn't account for the \"burstiness\" of language -- if we see the word \"Dracula\" in a text, we're probably going to see it again in that same text.  The occurrence of words are not independent random events; they are tightly coupled with each other. If we're trying to understanding the robust differences between two corpora, we might prefer to prioritize words that show up more frequently *everywhere* in corpus A (but not in corpus B) over those that show up only very frequently within narrow slice of A (such as one text in a genre, one chapter in a book, or one speaker when measuring the differences between policital parties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 (check-plus): One measure that does account for this burstiness is the adaptation by corpus linguistics of the non-parametric Mann-Whitney rank-sum test. The specific adaptation of this test for text is described in Kilgarriff 2001, section 2.3.  Implement this test using a fixed chunk size of 500 and the [scikit-learn mannwhitneyu function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html); what are the top ten terms in this measurement that are most republican and most democrat? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mann_whitney_analysis(dem_tokens, repub_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
